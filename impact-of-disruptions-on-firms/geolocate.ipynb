{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01414d1d",
   "metadata": {},
   "source": [
    "# Geocoding Brazilian Addresses\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook forward-geocodes Brazilian firms' addresses using the Google Geocoding API. It uses a cache file to avoid repeated API calls for the same address. It generates a `folium` map with markers for a sample of 1000 addresses to spot-check the geocoding results visually.\n",
    "\n",
    "## Output\n",
    "\n",
    "This notebook outputs 3 .csv files: `geocode_cache.csv`, `geocoded_data.csv`, and `geocoded_data_high_precision.csv`.\n",
    "\n",
    "- `geocode_cache.csv`: A cache file that stores the geocoding results for addresses already processed.\n",
    "- `geocoded_data.csv`: A file containing the original dataset with 5 extra columns: `full_address`, `lat`, `lng`, `status`, and `location_type`. `status` is OK if the address was successfully geocoded, and UNKNOWN_ERROR** or ZERO_RESULTS if not. `location_type` indicates the type of location returned by the API.\n",
    "- `geocoded_data_high_precision.csv`: `geocoded_data.csv` filtered to include only addresses with `location_type` as ROOFTOP or GEOMETRIC_CENTER, i.e. addresses with high-precision geocoding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85c4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: ISO-8859-1\n"
     ]
    }
   ],
   "source": [
    "encoding = ''\n",
    "file_path = 'data/raw/firm_ids_and_cities.csv'\n",
    "\n",
    "import chardet\n",
    "import os\n",
    "\n",
    "# 1. Determine the encoding of the input file\n",
    "with open(file_path, 'rb') as f:\n",
    "    detector = chardet.universaldetector.UniversalDetector()\n",
    "    for line in f:\n",
    "        detector.feed(line)\n",
    "        if detector.done:\n",
    "            break\n",
    "    detector.close()\n",
    "encoding = detector.result['encoding']\n",
    "print(f\"Detected encoding: {encoding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b49e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549082, 5)\n",
      "Index(['cnpj_cei', 'city_code', 'end_logradouro', 'city', 'year'], dtype='object')\n",
      "         cnpj_cei  city_code                              end_logradouro  \\\n",
      "0   2460658001930   110002.0                 RODOVIA BR 364, KM 523,5  .   \n",
      "1  84643881000159   110002.0                                AV JARU  S/N   \n",
      "2  34773267000133   110002.0                          AV RIO NEGRO 2260    \n",
      "3  84623768000101   110002.0  RUA A COM RODOVIA BR 421  CAIXA POSTAL 135   \n",
      "4  22861090000148   110002.0                 RODOVIA RO 01 KM 01 1 KM 01   \n",
      "\n",
      "                  city  year  \n",
      "0  Ariquemes, Rondônia  2003  \n",
      "1  Ariquemes, Rondônia  2003  \n",
      "2  Ariquemes, Rondônia  2003  \n",
      "3  Ariquemes, Rondônia  2003  \n",
      "4  Ariquemes, Rondônia  2003  \n",
      "Number of null values in each column: \n",
      "cnpj_cei           7\n",
      "city_code         27\n",
      "end_logradouro     9\n",
      "city              27\n",
      "year               0\n",
      "dtype: int64\n",
      "                                        full_address\n",
      "0  RODOVIA BR 364, KM 523,5 ., ARIQUEMES, RONDONI...\n",
      "1               AV JARU S/N, ARIQUEMES, RONDONIA, BR\n",
      "2         AV RIO NEGRO 2260, ARIQUEMES, RONDONIA, BR\n",
      "3  RUA A COM RODOVIA BR 421 CAIXA POSTAL 135, ARI...\n",
      "4  RODOVIA RO 01 KM 01 1 KM 01, ARIQUEMES, RONDON...\n",
      "Lookup shape: (193727, 1)\n",
      "Number of null values: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unidecode\n",
    "import re\n",
    "\n",
    "dtypes = {\n",
    "    \"cnpj_cei\": \"string\",\n",
    "    \"city_code\": \"float64\",\n",
    "    \"end_logradouro\": \"string\",\n",
    "    \"city\": \"string\",\n",
    "    \"year\": \"int64\",\n",
    "}\n",
    "\n",
    "# 2.1 Read the full panel (CSV or feather/pkl)\n",
    "df = pd.read_csv(file_path, encoding=encoding, dtype=dtypes)\n",
    "# or: df = pd.read_pickle(\"rais_data.pkl\")\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head())\n",
    "\n",
    "fill_na_values = {\n",
    "    \"cnpj_cei\": \"\",\n",
    "    \"city_code\": 0,\n",
    "    \"end_logradouro\": \"\",\n",
    "    \"city\": \"\",\n",
    "    \"year\": 0,\n",
    "}\n",
    "\n",
    "print(f\"Number of null values in each column: \\n{df.isna().sum()}\")\n",
    "\n",
    "# Fill null values with empty strings\n",
    "df.fillna(value=fill_na_values, inplace=True)\n",
    "\n",
    "# 2.2 Normalize & build full_address\n",
    "def clean_text(s):\n",
    "    s = str(s).strip()                        # trim whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s)               # collapse spaces\n",
    "    return s\n",
    "\n",
    "df[\"municipio\"], df[\"uf\"] = zip(*df[\"city\"]\n",
    "    .str.split(\",\", n=1)\n",
    "    .apply(lambda parts: (clean_text(parts[0]), clean_text(parts[1]) if len(parts)>1 else \"\")))\n",
    "\n",
    "df[\"pais\"] = \"BR\"\n",
    "df[\"end_logradouro\"] = df[\"end_logradouro\"].apply(clean_text)\n",
    "\n",
    "# Upper-case, remove accents\n",
    "df[\"full_address\"] = (\n",
    "    df[\"end_logradouro\"] + \", \" +\n",
    "    df[\"municipio\"]     + \", \" +\n",
    "    df[\"uf\"]            + \", \" +\n",
    "    df[\"pais\"]\n",
    ")\n",
    "df[\"full_address\"] = (\n",
    "    df[\"full_address\"]\n",
    "      .str.upper()\n",
    "      .apply(unidecode.unidecode)\n",
    ") \n",
    "\n",
    "# 2.3 Extract unique addresses lookup\n",
    "lookup = pd.DataFrame(df[\"full_address\"].unique(), columns=[\"full_address\"])\n",
    "print(lookup.head())\n",
    "print(f\"Lookup shape: {lookup.shape}\")\n",
    "print(f\"Number of null values: {lookup.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1e590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# 3.1 Sample 1,000 for trial geocode\n",
    "sample = lookup.sample(1000, random_state=1)\n",
    "\n",
    "# 3.2 Send `sample[\"full_address\"]` to a free geocoder - Mapbox\n",
    "#     Flag those with errors (e.g. missing numbers, “S/N”, bad municipio).\n",
    "#     Correct or drop in `lookup` before moving on.\n",
    "\n",
    "MAPBOX_URL = \"https://api.mapbox.com/search/geocode/v6/forward\"\n",
    "\n",
    "def geocode_address(address: str)->dict:\n",
    "    record = {}\n",
    "    params = {\n",
    "        \"q\": address,\n",
    "        \"access_token\": os.getenv(\"MAPBOX_ACCESS_TOKEN\"),\n",
    "        \"country\": \"BR\",\n",
    "        \"limit\": 1,\n",
    "\n",
    "    }\n",
    "    response = requests.get(MAPBOX_URL, params=params, headers={\"User-Agent\": \"kcao@bu.edu\"})\n",
    "    data = response.json()\n",
    "    if response.status_code == 200 and data and \"features\" in data and len(data[\"features\"]) > 0:\n",
    "        record = {\n",
    "            \"full_address\": address,\n",
    "            \"lat\": data[\"features\"][0][\"geometry\"][\"coordinates\"][1],\n",
    "            \"lng\": data[\"features\"][0][\"geometry\"][\"coordinates\"][0],\n",
    "            \"status\": \"OK\",\n",
    "            \"location_type\": data[\"features\"][0][\"properties\"][\"feature_type\"],\n",
    "        }\n",
    "    else:\n",
    "        record = {\n",
    "            \"full_address\": address,\n",
    "            \"lat\": None,\n",
    "            \"lng\": None,\n",
    "            \"status\": \"ERROR\",\n",
    "            \"location_type\": None,\n",
    "        }\n",
    "    if response.status_code == 429:\n",
    "        print(\"Rate limit exceeded. Please wait before making more requests.\")\n",
    "        time.sleep(5)\n",
    "    return record\n",
    "\n",
    "new_rows = []\n",
    "for address in sample[\"full_address\"]:\n",
    "    record = geocode_address(address)\n",
    "    new_rows.append(record)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "num_errors = sum(1 for row in new_rows if row[\"status\"] == \"ERROR\")\n",
    "print(f\"Number of errors: {num_errors}\")\n",
    "print(f\"Number of successful geocodes: {len(new_rows) - num_errors}\")\n",
    "error_rows = [row for row in new_rows if row[\"status\"] == \"ERROR\"]\n",
    "print(f\"Error rows: {error_rows}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e3cb0",
   "metadata": {},
   "source": [
    "## Sampling results\n",
    "\n",
    "Number of samples: 2000 \n",
    "\n",
    "Number of errors encountered: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88fc99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'full_address': 'RUA SIMAO BARBOSA 1208 EDWIGES LOJA 10, CANINDE, CEARA, BR',\n",
       "  'lat': -4.360424,\n",
       "  'lng': -39.313174,\n",
       "  'status': 'OK',\n",
       "  'location_type': 'address'},\n",
       " {'full_address': 'PANDIA CALOGENAS, BLUMENAU, SANTA CATARINA, BR',\n",
       "  'lat': -26.927142,\n",
       "  'lng': -49.062726,\n",
       "  'status': 'OK',\n",
       "  'location_type': 'street'},\n",
       " {'full_address': 'AV. PRINCESA ISABEL 10 AV ATLA 1020, RIO DE JANEIRO, RIO DE JANEIRO, BR',\n",
       "  'lat': -22.96442,\n",
       "  'lng': -43.173397,\n",
       "  'status': 'OK',\n",
       "  'location_type': 'address'},\n",
       " {'full_address': 'RUA JOSE ALVES BEZERRA 454, JABOATAO DOS GUARARAPES, PERNAMBUCO, BR',\n",
       "  'lat': -8.163556,\n",
       "  'lng': -34.937768,\n",
       "  'status': 'OK',\n",
       "  'location_type': 'address'},\n",
       " {'full_address': 'RUA MARACA, BELO HORIZONTE, MINAS GERAIS, BR',\n",
       "  'lat': -19.945241,\n",
       "  'lng': -43.946738,\n",
       "  'status': 'OK',\n",
       "  'location_type': 'street'},\n",
       " {'full_address': 'AVENIDA BASILEIA, RESENDE, RIO DE JANEIRO, BR',\n",
       "  'lat': -22.470257,\n",
       "  'lng': -44.46742,\n",
       "  'status': 'OK',\n",
       "  'location_type': 'street'},\n",
       " {'full_address': 'LOTGRANJAS RURAIS SN ., SALVADOR, BAHIA, BR',\n",
       "  'lat': -12.290962,\n",
       "  'lng': -38.944885,\n",
       "  'status': 'OK',\n",
       "  'location_type': 'street'},\n",
       " {'full_address': 'R JOAO BATISTA NOGUEIRA, GUARULHOS, SAO PAULO, BR',\n",
       "  'lat': -23.459988,\n",
       "  'lng': -46.463505,\n",
       "  'status': 'OK',\n",
       "  'location_type': 'street'},\n",
       " {'full_address': 'ROD SP 75 KM 48 PREDIO ILC SALA 5 C 48 KM 48, INDAIATUBA, SAO PAULO, BR',\n",
       "  'lat': -23.088585,\n",
       "  'lng': -47.21629,\n",
       "  'status': 'OK',\n",
       "  'location_type': 'place'},\n",
       " {'full_address': 'RODOVIA ANHANGUERA KM 111 5 ., SAO PAULO, SAO PAULO, BR',\n",
       "  'lat': -23.444487,\n",
       "  'lng': -46.797222,\n",
       "  'status': 'OK',\n",
       "  'location_type': 'locality'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.3 Inspect the sample geocode results\n",
    "new_rows[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa87dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 4.1 Load or initialize cache\n",
    "cache_dtypes = {\n",
    "    \"full_address\": \"string\",\n",
    "    \"lat\": \"float64\",\n",
    "    \"lng\": \"float64\",\n",
    "    \"status\": \"string\",\n",
    "    \"location_type\": \"string\",\n",
    "}\n",
    "cache_file = Path(\"geocode_cache.csv\")\n",
    "if cache_file.exists():\n",
    "    cache = pd.read_csv(cache_file, dtype=cache_dtypes)\n",
    "else:\n",
    "    cache = pd.DataFrame(columns=[\n",
    "        \"full_address\", \"lat\", \"lng\", \"status\", \"location_type\"\n",
    "        ], \n",
    "        dtype=cache_dtypes)\n",
    "    \n",
    "\n",
    "# 4.2 Geocoding loop\n",
    "API_KEY = os.getenv(\"GOOGLE_GEOCODING_API_KEY\")\n",
    "base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "new_rows = []\n",
    "\n",
    "MAX_RATE_LIMIT_RETRIES = 5\n",
    "\n",
    "CHECKPOINT = 1000\n",
    "\n",
    "for i, addr in enumerate(lookup[\"full_address\"]):\n",
    "    retry_count = 0\n",
    "    try:\n",
    "        if addr in cache[\"full_address\"].values:\n",
    "            continue  # already geocoded\n",
    "        num_api_requests+=1\n",
    "        params = {\n",
    "            \"address\": addr,\n",
    "            \"components\": \"country:BR\",\n",
    "            \"key\": API_KEY\n",
    "        }\n",
    "        resp = requests.get(base_url, params=params).json()\n",
    "        status = resp.get(\"status\")\n",
    "\n",
    "        if status == \"OK\" and resp[\"results\"]:\n",
    "            res = resp[\"results\"][0]\n",
    "            loc = res[\"geometry\"][\"location\"]\n",
    "            loc_type = res[\"geometry\"][\"location_type\"]\n",
    "            new_rows.append({\n",
    "                \"full_address\": addr,\n",
    "                \"lat\": loc[\"lat\"],\n",
    "                \"lng\": loc[\"lng\"],\n",
    "                \"status\": status,\n",
    "                \"location_type\": loc_type\n",
    "            })\n",
    "        else:\n",
    "            new_rows.append({\n",
    "                \"full_address\": addr,\n",
    "                \"lat\": None,\n",
    "                \"lng\": None,\n",
    "                \"status\": status,\n",
    "                \"location_type\": None\n",
    "            })\n",
    "\n",
    "        # Rate-limit + backoff\n",
    "        time.sleep(0.01) # ~100 requests/sec\n",
    "        if status == \"OVER_QUERY_LIMIT\":\n",
    "            retry_count += 1\n",
    "            if retry_count > MAX_RATE_LIMIT_RETRIES:\n",
    "                print(\"Max retries exceeded. Exiting.\")\n",
    "                exit(1)\n",
    "            print(\"Rate limit exceeded. Waiting for 5 seconds.\")\n",
    "            time.sleep(5)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Geocoding interrupted. Saving cache and exiting.\")\n",
    "        if new_rows:\n",
    "            cache = pd.concat([cache, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "            cache.to_csv(cache_file, index=False)\n",
    "        exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing address {addr}: {e}\")\n",
    "        exit(1)\n",
    "    finally:\n",
    "        if i % CHECKPOINT == 0: # Checkpoint and save cache every CHECKPOINT addresses processed\n",
    "            # print(f\"Processed {i} addresses. Cache size: {cache.shape[0]}\")\n",
    "            if new_rows:\n",
    "                cache = pd.concat([cache, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "                cache.to_csv(cache_file, index=False)\n",
    "                new_rows = []\n",
    "# 4.3 Append & save cache\n",
    "if new_rows:\n",
    "    cache = pd.concat([cache, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    cache.to_csv(cache_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2de9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cache size: 193727\n",
      "Number of errors: 19\n",
      "                                            full_address  lat  lng  \\\n",
      "5208   AV CRISTIANO MACHADO 4000 LJ ANCORA D, BELO HO...  NaN  NaN   \n",
      "10375        PRACA DAS FLORES 30, BARUERI, SAO PAULO, BR  NaN  NaN   \n",
      "11800  RUA GERONIMO CAYETANO GARCIA 176, FRANCISCO MO...  NaN  NaN   \n",
      "12885  AV CARLOS DRUMOND DE ANDRADE 434, LENCOIS PAUL...  NaN  NaN   \n",
      "18091        RUA SAO PEDRO S/N, SAO ROQUE, SAO PAULO, BR  NaN  NaN   \n",
      "32263  FAZENDA SANT ANA S/N, SAO JOAQUIM DA BARRA, SA...  NaN  NaN   \n",
      "32338  AVENIDA HEITOR VILA LOBOS 1172 SALA 4, SAO JOS...  NaN  NaN   \n",
      "46898  AV. DAS NACOES UNIDAS 4254, NOVO HAMBURGO, RIO...  NaN  NaN   \n",
      "54175  R SENADOR BENTO PEREIRA BUENO 129, JUNDIAI, SA...  NaN  NaN   \n",
      "64839  RUA ROMUALDO ANDREAZZI 492, CAMPINAS, SAO PAUL...  NaN  NaN   \n",
      "\n",
      "              status location_type  \n",
      "5208   UNKNOWN_ERROR          <NA>  \n",
      "10375   ZERO_RESULTS          <NA>  \n",
      "11800  UNKNOWN_ERROR          <NA>  \n",
      "12885  UNKNOWN_ERROR          <NA>  \n",
      "18091  UNKNOWN_ERROR          <NA>  \n",
      "32263  UNKNOWN_ERROR          <NA>  \n",
      "32338  UNKNOWN_ERROR          <NA>  \n",
      "46898  UNKNOWN_ERROR          <NA>  \n",
      "54175  UNKNOWN_ERROR          <NA>  \n",
      "64839  UNKNOWN_ERROR          <NA>  \n"
     ]
    }
   ],
   "source": [
    "# 5 Inspect the cache and check for errors\n",
    "print(f\"Final cache size: {cache.shape[0]}\")\n",
    "num_errors = cache[cache[\"status\"] != \"OK\"].shape[0]\n",
    "print(f\"Number of errors: {num_errors}\")\n",
    "\n",
    "error_df = cache[cache[\"status\"] != \"OK\"]\n",
    "if not error_df.empty:\n",
    "    print(error_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1f2aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Merge lat/lng into full panel\n",
    "df_geocoded = df.merge(cache, on=\"full_address\", how=\"left\")\n",
    "\n",
    "df_geocoded.to_csv(\"geocoded_data.csv\", index=False)\n",
    "\n",
    "# 6.2 Keep only high-precision hits if desired\n",
    "high_prec = [\"ROOFTOP\", \"RANGE_INTERPOLATED\"]\n",
    "df_high = df_geocoded[df_geocoded[\"location_type\"].isin(high_prec)]\n",
    "\n",
    "df_high.to_csv(\"geocoded_data_high_precision.csv\", index=False)\n",
    "\n",
    "# 6.3 Spot-check a random sample on a map (e.g. with folium)\n",
    "import folium\n",
    "m = folium.Map(location=[-15, -55], zoom_start=4)\n",
    "for _, row in df_high.sample(100).iterrows():\n",
    "    folium.CircleMarker(\n",
    "        [row.lat, row.lng], radius=2, color=\"blue\"\n",
    "    ).add_to(m)\n",
    "m.save(\"spotcheck.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d6bcbdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full geocoded data size: (549082, 13)\n",
      "Number of errors in full data: 60\n",
      "High-precision geocoded data size: (224670, 13)\n"
     ]
    }
   ],
   "source": [
    "# 7 Inspect the final geocoded data\n",
    "print(f\"Full geocoded data size: {df_geocoded.shape}\")\n",
    "num_errors_full = df_geocoded[df_geocoded[\"status\"] != \"OK\"].shape[0]\n",
    "print(f\"Number of errors in full data: {num_errors_full}\")\n",
    "\n",
    "print(f\"High-precision geocoded data size: {df_high.shape}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
