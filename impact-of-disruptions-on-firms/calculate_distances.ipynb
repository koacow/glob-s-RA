{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e98329",
   "metadata": {},
   "source": [
    "# Calculating Distances Between Brazillian Establishments and Natural Disasters\n",
    "\n",
    "## Overview\n",
    "This notebook calculates the distances between Brazilian establishments (from `geocoded_data_updt.csv`) and natural disasters (from `Brazil\\ Shocks.\\ 2000-2018.csv`). Distances are calculated using the Haversine formula, which accounts for the curvature of the Earth, and for all pairs of establishments and disasters.\n",
    "\n",
    "Note that the Haversine formula assumes that the Earth is a perfect sphere, which is not entirely accurate. However, for most practical purposes, this approximation is sufficient.\n",
    "\n",
    "## Output\n",
    "\n",
    "This notebook will output a single CSV file containing a single row for each establishment-disaster pair within the same year. The output file is an **inner join** of the two input files, meaning that only disaster-establishment pairs that happen in the same year will be included.\n",
    "The columns of the output file are as follows:\n",
    "- `est_id`: The ID of the establishment. This comes from the `cnpj_cei` column in the `geocoded_data_updt.csv` file. Note that there are some NAs in this column.\n",
    "- `disaster_id`: The ID of the disaster. This comes from the `geo_id` and `year` columns in the `Brazil\\ Shocks.\\ 2000-2018.csv` file. The format is `geo_id_year`.\n",
    "- `year`: The year of the disaster and the establishment.\n",
    "- `disaster_type`: The type of disaster. This comes from the `disastertype` column in the `Brazil\\ Shocks.\\ 2000-2018.csv` file.\n",
    "- `lat_est`: The latitude of the establishment. \n",
    "- `lon_est`: The longitude of the establishment.\n",
    "- `lat_disaster`: The latitude of the disaster.\n",
    "- `lon_disaster`: The longitude of the disaster.\n",
    "- `distance_km`: The distance between the establishment and the disaster in kilometers. This is calculated using the Haversine formula.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c327045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Detect the encoding of input files\n",
    "import chardet\n",
    "import os\n",
    "\n",
    "brazilian_shocks_path = \"/Users/koacow/BOSTON UNIVERSITY Dropbox/Ngoc Duy Khoa Cao/GLOB~S/Data/Natural Disasters/corrected_disaster_data.xlsx\"\n",
    "brazilian_est_path = os.path.join(os.getcwd(), 'geocoded_data/geocoded_data_updt.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12c79bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(287, 7)\n",
      "      year disaster_id        lat        lng disaster_type  total_deaths  \\\n",
      "66    2013   3036_2013 -22.587911 -43.326527         flood           4.0   \n",
      "67    2013   3037_2013 -22.587911 -43.326527         flood          30.0   \n",
      "2227  2000   2711_2000 -22.507438 -44.188600         flood          26.0   \n",
      "2228  2000   2915_2000 -22.441788 -44.490386         flood          26.0   \n",
      "2229  2000   3035_2000 -22.490089 -44.087939         flood          26.0   \n",
      "\n",
      "      total_damage  \n",
      "66          2000.0  \n",
      "67          1500.0  \n",
      "2227           NaN  \n",
      "2228           NaN  \n",
      "2229           NaN  \n",
      "(549082, 4)\n",
      "         est_id  year        lat        lng\n",
      "0  2.460658e+12  2003  -9.852406 -63.060539\n",
      "1  8.464388e+13  2003  -9.903970 -63.035419\n",
      "2  3.477327e+13  2003  -9.920243 -63.046216\n",
      "3  8.462377e+13  2003 -10.083945 -63.217735\n",
      "4  2.286109e+13  2003  -9.936345 -63.013974\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Load the data with only the necessary columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "shocks_df = pd.read_excel(brazilian_shocks_path, usecols=['year', 'geo_id', 'iso3', 'latitude', 'longitude', 'disastertype_x', 'Total Deaths', \"Total Damage ('000 US$)\"])\n",
    "est_df = pd.read_csv(brazilian_est_path, usecols=['cnpj_cei', 'year', 'lat', 'lng'])\n",
    "shocks_df = shocks_df[shocks_df['iso3'] == 'BRA']\n",
    "shocks_df = shocks_df[shocks_df['year'].isin(range(2000, 2019))]\n",
    "shocks_df = shocks_df[shocks_df['disastertype_x'].isin(['flood', 'storm', 'landslide', 'earthquake'])]\n",
    "# 2.2 Rename columns for consistency\n",
    "shocks_df['geo_id'] = shocks_df['geo_id'].astype(str)\n",
    "shocks_df['disaster_id'] = shocks_df['geo_id'] + '_' + shocks_df['year'].astype(str)\n",
    "shocks_df_cols = {\n",
    "    'year': 'year',\n",
    "    'disaster_id': 'disaster_id',\n",
    "    'latitude': 'lat',\n",
    "    'longitude': 'lng',\n",
    "    'disastertype_x': 'disaster_type',\n",
    "    'Total Deaths': 'total_deaths',\n",
    "    \"Total Damage ('000 US$)\": 'total_damage'\n",
    "}\n",
    "\n",
    "est_df_cols = {\n",
    "    'cnpj_cei': 'est_id',\n",
    "    'year': 'year',\n",
    "    'lat': 'lat',\n",
    "    'lng': 'lng'\n",
    "}\n",
    "\n",
    "shocks_df = shocks_df[shocks_df_cols.keys()]\n",
    "est_df = est_df[est_df_cols.keys()]\n",
    "shocks_df.rename(columns=shocks_df_cols, inplace=True)\n",
    "est_df.rename(columns=est_df_cols, inplace=True)\n",
    "\n",
    "print(shocks_df.shape)\n",
    "print(shocks_df.head())\n",
    "print(est_df.shape)\n",
    "print(est_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f139a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in shocks_df: total_deaths      9\n",
      "total_damage    148\n",
      "dtype: int64\n",
      "Missing values in est_df: est_id    7\n",
      "dtype: int64\n",
      "Number of rows in shocks_df with same (disaster_id): 0\n",
      "Number of rows in est_df with same (est_id, year): 0\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Check for missing values\n",
    "def check_missing_values(df):\n",
    "    missing_values = df.isna().sum()\n",
    "    return missing_values[missing_values > 0] if not missing_values[missing_values > 0].empty else None\n",
    "\n",
    "missing_vals_shocks = check_missing_values(shocks_df)\n",
    "missing_vals_est = check_missing_values(est_df)\n",
    "print(f\"Missing values in shocks_df: {missing_vals_shocks}\")\n",
    "print(f\"Missing values in est_df: {missing_vals_est}\")\n",
    "\n",
    "# 3.2 Check for duplicates\n",
    "\n",
    "duplicates_shocks = shocks_df.duplicated(subset=['disaster_id']).sum()\n",
    "duplicates_est = est_df.duplicated(subset=['est_id', 'year']).sum()\n",
    "print(f\"Number of rows in shocks_df with same (disaster_id): {duplicates_shocks}\")\n",
    "print(f\"Number of rows in est_df with same (est_id, year): {duplicates_est}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36af1ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape:\n",
      "Number of rows: 8999218\n",
      "Number of columns: 10\n",
      "   year disaster_id  lat_disaster  lng_disaster disaster_type  total_deaths  \\\n",
      "0  2013   3036_2013    -22.587911    -43.326527         flood           4.0   \n",
      "1  2013   3036_2013    -22.587911    -43.326527         flood           4.0   \n",
      "2  2013   3036_2013    -22.587911    -43.326527         flood           4.0   \n",
      "3  2013   3036_2013    -22.587911    -43.326527         flood           4.0   \n",
      "4  2013   3036_2013    -22.587911    -43.326527         flood           4.0   \n",
      "\n",
      "   total_damage        est_id    lat_est    lng_est  \n",
      "0        2000.0  2.916265e+12 -10.033735 -62.977755  \n",
      "1        2000.0  8.461554e+13  -9.908812 -63.035547  \n",
      "2        2000.0  6.900697e+12  -9.903959 -63.034621  \n",
      "3        2000.0  3.780605e+12  -9.898254 -63.034756  \n",
      "4        2000.0  4.082624e+12  -9.898254 -63.034756  \n"
     ]
    }
   ],
   "source": [
    "# 4.1 Merge the two dataframes on year\n",
    "merged_df = pd.merge(shocks_df, est_df, on='year', suffixes=('_disaster', '_est'))\n",
    "print(\"Merged DataFrame shape:\")\n",
    "print(\"Number of rows:\", merged_df.shape[0])\n",
    "print(\"Number of columns:\", merged_df.shape[1])\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ab868b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year disaster_id  lat_disaster  lng_disaster disaster_type  total_deaths  \\\n",
      "0  2013   3036_2013    -22.587911    -43.326527         flood           4.0   \n",
      "1  2013   3036_2013    -22.587911    -43.326527         flood           4.0   \n",
      "2  2013   3036_2013    -22.587911    -43.326527         flood           4.0   \n",
      "3  2013   3036_2013    -22.587911    -43.326527         flood           4.0   \n",
      "4  2013   3036_2013    -22.587911    -43.326527         flood           4.0   \n",
      "\n",
      "   total_damage        est_id    lat_est    lng_est  distance_km  \n",
      "0        2000.0  2.916265e+12 -10.033735 -62.977755  2514.201261  \n",
      "1        2000.0  8.461554e+13  -9.908812 -63.035547  2527.515565  \n",
      "2        2000.0  6.900697e+12  -9.903959 -63.034621  2527.753442  \n",
      "3        2000.0  3.780605e+12  -9.898254 -63.034756  2528.141185  \n",
      "4        2000.0  4.082624e+12  -9.898254 -63.034756  2528.141185  \n"
     ]
    }
   ],
   "source": [
    "from haversine import haversine\n",
    "\n",
    "def calculate_distance(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the distance between two geographical points using the Haversine formula. Returns the distance in kilometers.\n",
    "    \"\"\"\n",
    "    from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "    # Extract latitude and longitude from the row\n",
    "    lat1 = row['lat_disaster']\n",
    "    lon1 = row['lng_disaster']\n",
    "    lat2 = row['lat_est']\n",
    "    lon2 = row['lng_est']\n",
    "\n",
    "    dist = haversine((lat1, lon1), (lat2, lon2), unit='km')\n",
    "    return dist\n",
    "\n",
    "# 5. Calculate the distance between each establishment-disaster pair\n",
    "merged_df['distance_km'] = merged_df.apply(calculate_distance, axis=1)\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5df70620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in merged_df with same (est_id, disaster_id): 0\n"
     ]
    }
   ],
   "source": [
    "# 6. Check for duplicates in the merged DataFrame\n",
    "duplicates_merged = merged_df.duplicated(subset=['est_id', 'disaster_id']).sum()\n",
    "print(f\"Number of rows in merged_df with same (est_id, disaster_id): {duplicates_merged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e332fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Keep only the relevant columns\n",
    "columns_to_keep = ['year', 'disaster_id', 'est_id', 'lat_disaster', 'lng_disaster', 'lat_est', 'lng_est', 'disaster_type', 'distance_km', 'total_deaths', 'total_damage']\n",
    "merged_df = merged_df[columns_to_keep]\n",
    "merged_df['est_id'] = merged_df['est_id'].astype(str)\n",
    "\n",
    "# 7.2 Write the merged DataFrame to a CSV file\n",
    "output_path = os.path.join(os.getcwd(), 'geocoded_data/brazil_est_shock_distances.csv')\n",
    "merged_df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
