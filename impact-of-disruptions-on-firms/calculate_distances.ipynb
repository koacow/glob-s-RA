{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e98329",
   "metadata": {},
   "source": [
    "# Calculating Distances Between Brazillian Establishments and Natural Disasters\n",
    "\n",
    "## Overview\n",
    "This notebook calculates the distances between Brazilian establishments (from `geocoded_data_updt.csv`) and natural disasters (from `Brazil\\ Shocks.\\ 2000-2018.csv`). Distances are calculated using the Haversine formula, which accounts for the curvature of the Earth, and for all pairs of establishments and disasters.\n",
    "\n",
    "Note that the Haversine formula assumes that the Earth is a perfect sphere, which is not entirely accurate. However, for most practical purposes, this approximation is sufficient.\n",
    "\n",
    "## Output\n",
    "\n",
    "This notebook will output a single CSV file containing a single row for each establishment-disaster pair within the same year. The output file is an **inner join** of the two input files, meaning that only disaster-establishment pairs that happen in the same year will be included.\n",
    "The columns of the output file are as follows:\n",
    "- `est_id`: The ID of the establishment. This comes from the `cnpj_cei` column in the `geocoded_data_updt.csv` file. Note that there are some NAs in this column.\n",
    "- `disaster_id`: The ID of the disaster. This comes from the `geo_id` and `year` columns in the `Brazil\\ Shocks.\\ 2000-2018.csv` file. The format is `geo_id_year`.\n",
    "- `year`: The year of the disaster and the establishment.\n",
    "- `disaster_type`: The type of disaster. This comes from the `disastertype` column in the `Brazil\\ Shocks.\\ 2000-2018.csv` file.\n",
    "- `lat_est`: The latitude of the establishment. \n",
    "- `lon_est`: The longitude of the establishment.\n",
    "- `lat_disaster`: The latitude of the disaster.\n",
    "- `lon_disaster`: The longitude of the disaster.\n",
    "- `distance_km`: The distance between the establishment and the disaster in kilometers. This is calculated using the Haversine formula.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c327045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding of /Users/koacow/repos/glob-s-RA/impact-of-disruptions-on-firms/geocoded_data/geocoded_data_updt.csv: utf-8\n"
     ]
    }
   ],
   "source": [
    "# 1. Detect the encoding of input files\n",
    "import chardet\n",
    "import os\n",
    "\n",
    "brazilian_shocks_path = os.path.join(os.getcwd(), 'geocoded_data/Brazil Shocks. 2000-2018.xlsx')\n",
    "brazilian_est_path = os.path.join(os.getcwd(), 'geocoded_data/geocoded_data_updt.csv')\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "    return encoding\n",
    "\n",
    "brazilian_firms_encoding = detect_encoding(brazilian_est_path)\n",
    "print(f\"Encoding of {brazilian_est_path}: {brazilian_firms_encoding}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12c79bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(289, 5)\n",
      "   year disaster_id        lat        lng disaster_type\n",
      "0  1999  29236_1999 -22.265413 -48.732761         flood\n",
      "1  1999  40143_1999 -12.903113 -38.443985     landslide\n",
      "2  2000   2711_2000 -22.507438 -44.188600         flood\n",
      "3  2000   2915_2000 -22.441788 -44.490386         flood\n",
      "4  2000   3035_2000 -22.490089 -44.087939         flood\n",
      "(549082, 4)\n",
      "         est_id  year        lat        lng\n",
      "0  2.460658e+12  2003  -9.852406 -63.060539\n",
      "1  8.464388e+13  2003  -9.903970 -63.035419\n",
      "2  3.477327e+13  2003  -9.920243 -63.046216\n",
      "3  8.462377e+13  2003 -10.083945 -63.217735\n",
      "4  2.286109e+13  2003  -9.936345 -63.013974\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Load the data with only the necessary columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "shocks_df = pd.read_excel(brazilian_shocks_path, usecols=['year', 'geo_id', 'latitude', 'longitude', 'disastertype'])\n",
    "est_df = pd.read_csv(brazilian_est_path, encoding=brazilian_firms_encoding, usecols=['cnpj_cei', 'year', 'lat', 'lng'])\n",
    "\n",
    "# 2.2 Rename columns for consistency\n",
    "shocks_df['geo_id'] = shocks_df['geo_id'].astype(str)\n",
    "shocks_df['disaster_id'] = shocks_df['geo_id'] + '_' + shocks_df['year'].astype(str)\n",
    "shocks_df_cols = {\n",
    "    'year': 'year',\n",
    "    'disaster_id': 'disaster_id',\n",
    "    'latitude': 'lat',\n",
    "    'longitude': 'lng',\n",
    "    'disastertype': 'disaster_type'\n",
    "}\n",
    "\n",
    "est_df_cols = {\n",
    "    'cnpj_cei': 'est_id',\n",
    "    'year': 'year',\n",
    "    'lat': 'lat',\n",
    "    'lng': 'lng'\n",
    "}\n",
    "\n",
    "shocks_df = shocks_df[shocks_df_cols.keys()]\n",
    "est_df = est_df[est_df_cols.keys()]\n",
    "shocks_df.rename(columns=shocks_df_cols, inplace=True)\n",
    "est_df.rename(columns=est_df_cols, inplace=True)\n",
    "\n",
    "print(shocks_df.shape)\n",
    "print(shocks_df.head())\n",
    "print(est_df.shape)\n",
    "print(est_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f139a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in shocks_df: None\n",
      "Missing values in est_df: est_id    7\n",
      "dtype: int64\n",
      "Number of rows in shocks_df with same (disaster_id): 0\n",
      "Number of rows in est_df with same (est_id, year): 0\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Check for missing values\n",
    "def check_missing_values(df):\n",
    "    missing_values = df.isnull().sum()\n",
    "    return missing_values[missing_values > 0] if not missing_values[missing_values > 0].empty else None\n",
    "\n",
    "missing_vals_shocks = check_missing_values(shocks_df)\n",
    "missing_vals_est = check_missing_values(est_df)\n",
    "print(f\"Missing values in shocks_df: {missing_vals_shocks}\")\n",
    "print(f\"Missing values in est_df: {missing_vals_est}\")\n",
    "\n",
    "# 3.2 Check for duplicates\n",
    "\n",
    "duplicates_shocks = shocks_df.duplicated(subset=['disaster_id']).sum()\n",
    "duplicates_est = est_df.duplicated(subset=['est_id', 'year']).sum()\n",
    "print(f\"Number of rows in shocks_df with same (disaster_id): {duplicates_shocks}\")\n",
    "print(f\"Number of rows in est_df with same (est_id, year): {duplicates_est}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36af1ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame shape:\n",
      "Number of rows: 8999218\n",
      "Number of columns: 8\n",
      "   year disaster_id  lat_disaster  lng_disaster disaster_type        est_id  \\\n",
      "0  2003   2716_2003    -19.900563    -43.958439         flood  2.460658e+12   \n",
      "1  2003   2716_2003    -19.900563    -43.958439         flood  8.464388e+13   \n",
      "2  2003   2716_2003    -19.900563    -43.958439         flood  3.477327e+13   \n",
      "3  2003   2716_2003    -19.900563    -43.958439         flood  8.462377e+13   \n",
      "4  2003   2716_2003    -19.900563    -43.958439         flood  2.286109e+13   \n",
      "\n",
      "     lat_est    lng_est  \n",
      "0  -9.852406 -63.060539  \n",
      "1  -9.903970 -63.035419  \n",
      "2  -9.920243 -63.046216  \n",
      "3 -10.083945 -63.217735  \n",
      "4  -9.936345 -63.013974  \n"
     ]
    }
   ],
   "source": [
    "# 4.1 Merge the two dataframes on year\n",
    "merged_df = pd.merge(shocks_df, est_df, on='year', suffixes=('_disaster', '_est'))\n",
    "print(\"Merged DataFrame shape:\")\n",
    "print(\"Number of rows:\", merged_df.shape[0])\n",
    "print(\"Number of columns:\", merged_df.shape[1])\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ab868b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year disaster_id  lat_disaster  lng_disaster disaster_type        est_id  \\\n",
      "0  2003   2716_2003    -19.900563    -43.958439         flood  2.460658e+12   \n",
      "1  2003   2716_2003    -19.900563    -43.958439         flood  8.464388e+13   \n",
      "2  2003   2716_2003    -19.900563    -43.958439         flood  3.477327e+13   \n",
      "3  2003   2716_2003    -19.900563    -43.958439         flood  8.462377e+13   \n",
      "4  2003   2716_2003    -19.900563    -43.958439         flood  2.286109e+13   \n",
      "\n",
      "     lat_est    lng_est  distance_km  \n",
      "0  -9.852406 -63.060539  2333.858418  \n",
      "1  -9.903970 -63.035419  2328.562299  \n",
      "2  -9.920243 -63.046216  2328.656341  \n",
      "3 -10.083945 -63.217735  2335.658212  \n",
      "4  -9.936345 -63.013974  2324.705418  \n"
     ]
    }
   ],
   "source": [
    "from haversine import haversine\n",
    "\n",
    "def calculate_distance(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the distance between two geographical points using the Haversine formula. Returns the distance in kilometers.\n",
    "    \"\"\"\n",
    "    from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "    # Extract latitude and longitude from the row\n",
    "    lat1 = row['lat_disaster']\n",
    "    lon1 = row['lng_disaster']\n",
    "    lat2 = row['lat_est']\n",
    "    lon2 = row['lng_est']\n",
    "\n",
    "    dist = haversine((lat1, lon1), (lat2, lon2), unit='km')\n",
    "    return dist\n",
    "\n",
    "# 5. Calculate the distance between each establishment-disaster pair\n",
    "merged_df['distance_km'] = merged_df.apply(calculate_distance, axis=1)\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5df70620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in merged_df with same (est_id, disaster_id): 0\n"
     ]
    }
   ],
   "source": [
    "# 6. Check for duplicates in the merged DataFrame\n",
    "duplicates_merged = merged_df.duplicated(subset=['est_id', 'disaster_id']).sum()\n",
    "print(f\"Number of rows in merged_df with same (est_id, disaster_id): {duplicates_merged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e332fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Keep only the relevant columns\n",
    "columns_to_keep = ['year', 'disaster_id', 'est_id', 'lat_disaster', 'lng_disaster', 'lat_est', 'lng_est', 'disaster_type', 'distance_km']\n",
    "merged_df = merged_df[columns_to_keep]\n",
    "merged_df['est_id'] = merged_df['est_id'].astype(str)\n",
    "\n",
    "# 7.2 Write the merged DataFrame to a CSV file\n",
    "output_path = os.path.join(os.getcwd(), 'geocoded_data/brazil_est_shock_distances.csv')\n",
    "merged_df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
